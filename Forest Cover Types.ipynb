{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rborra/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importing required Python packages\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier  \n",
    "\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120, 55) (565892, 54)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Reading the data into Python Dataframes\n",
    "traincsv=pd.read_csv('train.csv', index_col='Id')\n",
    "testcsv=pd.read_csv('test.csv', index_col='Id')\n",
    "\n",
    "print traincsv.shape, testcsv.shape\n",
    "print type(traincsv), type(testcsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12096, 54) (3024, 54) (12096,) (3024,)\n",
      "<type 'numpy.ndarray'> <type 'numpy.ndarray'> <type 'numpy.ndarray'> <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data in the ratio 80:20, the data is converted from Dataframes to numpy ndarray\n",
    "X_train, X_test, y_train, y_test = train_test_split(traincsv.iloc[:,:-1].values, traincsv.iloc[:,-1:].values.ravel(), \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "print X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "print type(X_train), type(X_test), type(y_train), type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=1, missing=None, n_estimators=1100, nthread=-1,\n",
       "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting data to Classifiers, used Max Depth and Estimators based on experience and other factors\n",
    "RFC1=RandomForestClassifier(max_depth=6, n_estimators=800)\n",
    "RFC1.fit(X_train, y_train)\n",
    "\n",
    "RFC2=RandomForestClassifier(max_depth=14, n_estimators=1400)\n",
    "RFC2.fit(X_train, y_train)\n",
    "\n",
    "RFC3=RandomForestClassifier(max_depth=10, n_estimators=1100)\n",
    "RFC3.fit(X_train, y_train)\n",
    "\n",
    "ETC1= ExtraTreesClassifier(max_depth=6, n_estimators=800)\n",
    "ETC1.fit(X_train, y_train)\n",
    "\n",
    "ETC2= ExtraTreesClassifier(max_depth=14, n_estimators=1400)\n",
    "ETC2.fit(X_train, y_train)\n",
    "\n",
    "ETC3= ExtraTreesClassifier(max_depth=10, n_estimators=1100)\n",
    "ETC3.fit(X_train, y_train)\n",
    "\n",
    "GBC1= GradientBoostingClassifier(max_depth=6, n_estimators=800)\n",
    "GBC1.fit(X_train, y_train)\n",
    "\n",
    "GBC2= GradientBoostingClassifier(max_depth=14, n_estimators=1400)\n",
    "GBC2.fit(X_train, y_train)\n",
    "\n",
    "GBC3= GradientBoostingClassifier(max_depth=10, n_estimators=1100)\n",
    "GBC3.fit(X_train, y_train)\n",
    "\n",
    "XGB1= XGBClassifier(max_depth=6, n_estimators=800)\n",
    "XGB1.fit(X_train, y_train)\n",
    "\n",
    "XGB2= XGBClassifier(max_depth=14, n_estimators=1400)\n",
    "XGB2.fit(X_train, y_train)\n",
    "\n",
    "XGB3= XGBClassifier(max_depth=10, n_estimators=1100)\n",
    "XGB3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Stacking\n",
    "estimators = [RFC1,RFC2,RFC3, ETC1,ETC2,ETC3, GBC1,GBC2,GBC3, XGB1,XGB2,XGB3]\n",
    "\n",
    "# Creating empty lists to hold Predictions of the Classifiers\n",
    "X_blend_train   =  []\n",
    "X_blend_test    =  []\n",
    "X_blend_testcsv =  []\n",
    "\n",
    "# The lists are appended with the Classifier predictions, the list will have 12 arrays \n",
    "# which are predictions of 12 classifiers.\n",
    "for i, est in enumerate(estimators):\n",
    "    #print i , est\n",
    "    X_blend_train.append(est.predict(X_train))\n",
    "    X_blend_test.append(est.predict(X_test))\n",
    "    X_blend_testcsv.append(est.predict(testcsv.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12 12\n",
      "<type 'list'> <type 'list'> <type 'list'>\n",
      "[array([3, 5, 2, ..., 1, 2, 7]), array([3, 5, 2, ..., 2, 5, 7]), array([3, 5, 2, ..., 2, 5, 7]), array([3, 5, 2, ..., 5, 1, 7]), array([3, 5, 2, ..., 2, 5, 7]), array([3, 5, 2, ..., 2, 1, 7]), array([3, 5, 2, ..., 2, 5, 7]), array([3, 5, 2, ..., 2, 5, 7]), array([3, 5, 2, ..., 2, 5, 7]), array([3, 5, 2, ..., 2, 5, 7]), array([3, 5, 2, ..., 2, 5, 7]), array([3, 5, 2, ..., 2, 5, 7])]\n",
      "[array([3, 6, 7, ..., 5, 4, 4]), array([6, 6, 1, ..., 5, 4, 4]), array([3, 6, 1, ..., 5, 4, 4]), array([4, 6, 7, ..., 5, 4, 4]), array([4, 6, 1, ..., 5, 4, 4]), array([4, 6, 1, ..., 5, 4, 4]), array([6, 6, 1, ..., 5, 4, 4]), array([6, 6, 1, ..., 5, 4, 4]), array([6, 6, 1, ..., 5, 4, 4]), array([6, 3, 1, ..., 5, 4, 4]), array([6, 3, 1, ..., 5, 4, 4]), array([6, 3, 1, ..., 5, 4, 4])]\n",
      "[array([2, 2, 2, ..., 3, 3, 3]), array([2, 2, 2, ..., 3, 3, 3]), array([2, 2, 2, ..., 3, 3, 3]), array([2, 2, 2, ..., 3, 3, 3]), array([2, 2, 2, ..., 3, 3, 3]), array([2, 2, 2, ..., 3, 3, 3]), array([1, 1, 1, ..., 3, 3, 3]), array([5, 5, 1, ..., 3, 3, 3]), array([1, 1, 1, ..., 3, 3, 3]), array([5, 5, 1, ..., 3, 3, 3]), array([5, 5, 1, ..., 3, 3, 3]), array([5, 1, 1, ..., 3, 3, 3])]\n",
      "12096 3024 565892\n",
      "(12096, 12) (3024, 12) (565892, 12)\n",
      "<type 'numpy.ndarray'> <type 'numpy.ndarray'> <type 'numpy.ndarray'>\n",
      "---------------------------------\n",
      "[[3 3 3 ..., 3 3 3]\n",
      " [5 5 5 ..., 5 5 5]\n",
      " [2 2 2 ..., 2 2 2]\n",
      " ..., \n",
      " [1 2 2 ..., 2 2 2]\n",
      " [2 5 5 ..., 5 5 5]\n",
      " [7 7 7 ..., 7 7 7]]\n",
      "---------------------------------\n",
      "[[3 6 3 ..., 6 6 6]\n",
      " [6 6 6 ..., 3 3 3]\n",
      " [7 1 1 ..., 1 1 1]\n",
      " ..., \n",
      " [5 5 5 ..., 5 5 5]\n",
      " [4 4 4 ..., 4 4 4]\n",
      " [4 4 4 ..., 4 4 4]]\n",
      "---------------------------------\n",
      "[[2 2 2 ..., 5 5 5]\n",
      " [2 2 2 ..., 5 5 1]\n",
      " [2 2 2 ..., 1 1 1]\n",
      " ..., \n",
      " [3 3 3 ..., 3 3 3]\n",
      " [3 3 3 ..., 3 3 3]\n",
      " [3 3 3 ..., 3 3 3]]\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print len(X_blend_train), len(X_blend_test), len(X_blend_testcsv)\n",
    "print type(X_blend_train), type(X_blend_test), type(X_blend_testcsv)\n",
    "\n",
    "print X_blend_train\n",
    "print X_blend_test\n",
    "print X_blend_testcsv\n",
    "\n",
    "# Transforming the Lists which have the Classifier predictions\n",
    "X_blend_train = np.array(X_blend_train).T\n",
    "X_blend_test = np.array(X_blend_test).T\n",
    "X_blend_testcsv = np.array(X_blend_testcsv).T\n",
    "\n",
    "print len(X_blend_train), len(X_blend_test), len(X_blend_testcsv)\n",
    "print X_blend_train.shape, X_blend_test.shape, X_blend_testcsv.shape\n",
    "print type(X_blend_train), type(X_blend_test), type(X_blend_testcsv)\n",
    "\n",
    "print '---------------------------------'\n",
    "print X_blend_train\n",
    "print '---------------------------------'\n",
    "print X_blend_test\n",
    "print '---------------------------------'\n",
    "print X_blend_testcsv\n",
    "print '---------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB2= XGBClassifier(max_depth=10, n_estimators=1000).fit(X_blend_train, y_train) \n",
    "\n",
    "#Make a copy of the test.csv \n",
    "test_data=testcsv.copy()\n",
    "\n",
    "#Run Predictions on test.csv \n",
    "test_data['Cover_Type']=XGB2.predict(X_blend_testcsv) \n",
    "\n",
    "#Create Submissions csv file \n",
    "test_data=test_data['Cover_Type'] \n",
    "test_data.to_csv('SampleSubmission20', header=True) \n",
    "print 'Complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
